# -*- coding: utf-8 -*-
"""Smart AI Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkbGZGaRyip_i48MedHZzBOk6ncmwpaB

# Install Dependencies
"""

!pip install -q transformers gradio torch torchvision pillow accelerate sentencepiece datasets moviepy

"""# Import Libraries"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from PIL import Image
import gradio as gr
from moviepy.editor import VideoFileClip, AudioFileClip
import warnings

warnings.filterwarnings("ignore")

"""# Configuration"""

import torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE.upper()}")

"""# Personalities"""

PERSONALITIES = {
    "Friendly Tutor": "You are a helpful, patient, and concise tutor. Keep explanations clear and give simple examples.",
    "Funny Companion": "You are witty, light-hearted, and sprinkle short jokes. Keep it friendly and non-offensive.",
    "Professional Support Agent": "You are a formal and helpful customer support agent. Use polite, structured responses and provide steps/links when appropriate.",
    "Concise Assistant": "You are brief and to the point. Give short answers and optionally bullet steps if needed.",
    "Motivational Coach": "You are encouraging and inspiring. Give positive advice, uplifting messages, and motivational tips.",
    "Technical Expert": "You provide detailed technical explanations and programming guidance. Be precise and accurate.",
    "Creative Writer": "You help generate stories, poems, or creative writing. Be imaginative and descriptive.",
    "Health Advisor": "You give general health tips, wellness advice, and fitness suggestions politely.",
    "Business Consultant": "You provide professional business and strategy advice, keeping a formal tone.",
    "Travel Guide": "You are a travel expert, giving tips, sightseeing suggestions, and local recommendations."
}

# Optionally, print the personalities
print("Available Personalities:")
for idx, key in enumerate(PERSONALITIES.keys(), 1):
    print(f"{idx}. {key}")

"""# Emoji Replacement Function"""

EMOJI_MAP = {
    ":smile:": "ğŸ˜Š",
    ":thumbs_up:": "ğŸ‘",
    ":sad:": "ğŸ˜¢",
    ":rocket:": "ğŸš€",
}

def apply_emoji(text, emoji_enabled=True):
    if not emoji_enabled:
        return text
    for k, v in EMOJI_MAP.items():
        text = text.replace(k, v)
    return text

"""# Load Models"""

def load_dialogpt():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
    model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
    model.to(DEVICE)
    return tokenizer, model

tokenizer, model = load_dialogpt()

# Pipelines
img_caption_pipeline = None
asr_pipeline = None
summarizer_pipeline = None

def load_img_caption_pipeline():
    global img_caption_pipeline
    if img_caption_pipeline is None:
        img_caption_pipeline = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning", device=0 if DEVICE=="cuda" else -1)
    return img_caption_pipeline

def load_asr_pipeline():
    global asr_pipeline
    if asr_pipeline is None:
        asr_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-small", device=0 if DEVICE=="cuda" else -1)
    return asr_pipeline

def load_summarizer_pipeline():
    global summarizer_pipeline
    if summarizer_pipeline is None:
        summarizer_pipeline = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if DEVICE=="cuda" else -1)
    return summarizer_pipeline

"""# Long Media Handling"""

def transcribe_and_summarize_long_audio(audio_path, chunk_duration=60, max_chunk_words=300):
    asr = load_asr_pipeline()
    summarizer = load_summarizer_pipeline()

    audio_clip = AudioFileClip(audio_path)
    total_duration = int(audio_clip.duration)
    progressive_summary = ""

    for start in range(0, total_duration, chunk_duration):
        end = min(start + chunk_duration, total_duration)
        chunk_path = f"temp_chunk_{start}_{end}.wav"
        audio_clip.subclip(start, end).write_audiofile(chunk_path, verbose=False, logger=None)
        chunk_text = asr(chunk_path).get("text", "")
        os.remove(chunk_path)
        combined_text = progressive_summary + " " + chunk_text if progressive_summary else chunk_text
        progressive_summary = summarize_text(combined_text, max_chunk_words=max_chunk_words)

    audio_clip.close()
    return progressive_summary

def summarize_text(text, max_chunk_words=300):
    summarizer = load_summarizer_pipeline()
    words = text.split()
    summaries = []
    for i in range(0, len(words), max_chunk_words):
        chunk = " ".join(words[i:i+max_chunk_words])
        summary = summarizer(chunk, max_length=60, min_length=20, do_sample=False)[0]['summary_text']
        summaries.append(summary)
    return " ".join(summaries)

"""# Chatbot Logic"""

MAX_HISTORY_TURNS = 8

def build_context(history, personality_prompt=None):
    parts = []
    if personality_prompt:
        parts.append(personality_prompt)
    for u, b in history[-MAX_HISTORY_TURNS:]:
        parts.append("User: " + u)
        parts.append("Bot: " + b)
    return " \n ".join(parts)

def chatbot_multimodal_input(user_text, history, personality, emoji_enabled, uploaded_file, temperature, max_new_tokens):
    user_text = apply_emoji(user_text, emoji_enabled)

    if uploaded_file is not None:
        filename = uploaded_file.name.lower()
        if filename.endswith((".png", ".jpg", ".jpeg")):
            try:
                img_pipeline = load_img_caption_pipeline()
                caption = img_pipeline(uploaded_file)[0]['generated_text']
                user_text += f" [Image description: {caption}]"
            except:
                user_text += " [Image description failed]"
        elif filename.endswith((".mp3", ".wav", ".m4a")):
            try:
                summary = transcribe_and_summarize_long_audio(uploaded_file.name)
                user_text += f" [Audio summary: {summary}]"
            except:
                user_text += " [Audio transcription failed]"
        elif filename.endswith((".mp4", ".mov")):
            try:
                clip = VideoFileClip(uploaded_file.name)
                audio_path = "temp_audio.wav"
                clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
                summary = transcribe_and_summarize_long_audio(audio_path)
                user_text += f" [Video summary: {summary}]"
                clip.close()
                os.remove(audio_path)
            except:
                user_text += " [Video transcription failed]"

    personality_prompt = PERSONALITIES.get(personality, None)
    context_text = build_context(history, personality_prompt)
    full_input = (context_text + " \nUser: " + user_text + " \nBot: ") if context_text else ("User: " + user_text + " \nBot: ")

    input_ids = tokenizer.encode(full_input + tokenizer.eos_token, return_tensors="pt").to(DEVICE)
    output_ids = model.generate(input_ids, do_sample=True, top_k=50, top_p=0.95, temperature=float(temperature), max_new_tokens=int(max_new_tokens), pad_token_id=tokenizer.eos_token_id)

    bot_response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()
    if not bot_response:
        bot_response = "I'm sorry â€” I couldn't generate a response."

    history.append((user_text, bot_response))
    if len(history) > MAX_HISTORY_TURNS:
        history = history[-MAX_HISTORY_TURNS:]

    return history, history

"""# Professional UI with Dark Mode & Floating File Preview"""

with gr.Blocks(css="""
    body {background-color: #1e1e2f; color:#fff;}
    .gradio-container {border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.3);}
    .chatbot-message-user {background-color:#4A90E2; color:white; border-radius:12px; padding:8px; margin:4px; max-width:70%;}
    .chatbot-message-bot {background-color:#333444; color:white; border-radius:12px; padding:8px; margin:4px; max-width:70%;}
    .file-preview {position:fixed; top:100px; right:20px; width:200px; padding:10px; background:#222233; border-radius:10px; box-shadow:0 2px 10px rgba(0,0,0,0.5);}
""") as demo:
    gr.Markdown("<h1 style='text-align:center; color:#FFD700;'>ğŸŒŒ Smart AI Chatbot ğŸŒŒ</h1>")

    with gr.Row():
        with gr.Column(scale=3):
            chatbot_ui = gr.Chatbot(label="Chatbot", elem_id="chatbot_ui")
            multimodal_input = gr.File(label="Type text or upload image/audio/video", file_types=[".png",".jpg",".jpeg",".mp3",".wav",".m4a",".mp4",".mov"], type="filepath")
            user_text = gr.Textbox(placeholder="Or type your message here...", label="Message")
            send_btn = gr.Button("Send", variant="primary")
        with gr.Column(scale=1):
            personality = gr.Dropdown(list(PERSONALITIES.keys()), value=list(PERSONALITIES.keys())[0], label="Personality")
            emoji_toggle = gr.Checkbox(label="Enable emoji replacements", value=True)
            temp_slider = gr.Slider(0.1, 1.2, value=0.8, step=0.05, label="Temperature")
            max_tokens_slider = gr.Slider(20, 200, value=80, step=1, label="Max new tokens")
            clear_btn = gr.Button("Clear Chat", variant="secondary")

    state = gr.State([])

    def submit(user_text_val, multimodal_file, state, personality, emoji_toggle, temp, max_new):
        input_text = user_text_val or ""
        display_history, new_state = chatbot_multimodal_input(input_text, state or [], personality, emoji_toggle, multimodal_file, temp, max_new)
        return display_history, new_state, ""

    send_btn.click(fn=submit, inputs=[user_text, multimodal_input, state, personality, emoji_toggle, temp_slider, max_tokens_slider], outputs=[chatbot_ui, state, user_text])
    user_text.submit(fn=submit, inputs=[user_text, multimodal_input, state, personality, emoji_toggle, temp_slider, max_tokens_slider], outputs=[chatbot_ui, state, user_text])
    clear_btn.click(lambda: ([], []), None, [chatbot_ui, state])
     # Footer
    gr.HTML("""
<div align="center">

**ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’» **
<h3>ğŸ§‘â€ğŸ’» Developed by :Md. Ferdaus HossenğŸ§‘â€ğŸ’»</h3>
<h5>Junior AI/ML Engineer at Zensoft Lab</h5>

<p>
  <a href="https://github.com/Ferdaus71" target="_blank" style="margin-right:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" width="25" height="25" alt="GitHub">
  </a>
  <a href="https://www.linkedin.com/in/ferdaus70/" target="_blank" style="margin-left:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg" width="25" height="25" alt="LinkedIn">
  </a>
</p>

</div>
""")

"""# Launch App"""

demo.launch(share=True)